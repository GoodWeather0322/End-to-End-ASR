{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from kaldiio import ReadHelper\n",
    "import json\n",
    "import os \n",
    "import time\n",
    "from tqdm import tqdm_notebook, notebook\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "espnet_path = Path('espnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egs_path = Path('egs/aishell/asr1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = 'dump'\n",
    "train_path = 'train_sp'\n",
    "dev_path = 'dev'\n",
    "test_path = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = espnet_path / egs_path / 'data/lang_1char/train_sp_units.txt'\n",
    "vocab = {}\n",
    "with open(vocab_file) as fp:\n",
    "    for line in fp:\n",
    "        word, idx = line.strip().split()\n",
    "        vocab[word] = idx\n",
    "        \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = 4\n",
    "token2id = {'<blank>':0, '<pad>':1, '<sos>':2, '<eos>':3}\n",
    "for word, _ in vocab.items():\n",
    "    if word not in token2id:\n",
    "        token2id[word] = new_id\n",
    "        new_id += 1\n",
    "\n",
    "id2token = {v:k for k, v in token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = {}\n",
    "dev_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for (set_path, dataset) in zip([dev_path, test_path], [dev_data, test_data]):\n",
    "    set_dump_path = espnet_path / egs_path / dump_path / set_path / 'deltafalse' \n",
    "    with open(set_dump_path / 'data.json') as fp:\n",
    "        json_data = json.load(fp)\n",
    "\n",
    "    feats = {}\n",
    "    pbar = notebook.tqdm(total=len(list(set_dump_path.glob('feats.*.ark'))))\n",
    "    for feats_file in set_dump_path.glob('feats.*.ark'):\n",
    "        with ReadHelper('ark:'+str(feats_file)) as reader:\n",
    "            for key, numpy_array in reader:\n",
    "                feats[key] = torch.from_numpy(numpy_array)\n",
    "          \n",
    "        pbar.update(1)\n",
    "        \n",
    "    \n",
    "    for key, value in json_data['utts'].items():\n",
    "        if key in feats:\n",
    "            feature = feats[key]\n",
    "            text = json_data['utts'][key]['output'][0]['text']\n",
    "            token = []\n",
    "            token_id = []\n",
    "            for char in text:\n",
    "                if char == ' ':\n",
    "                    token.append('<space>')\n",
    "                else:\n",
    "                    if char in vocab:\n",
    "                        token.append(char)\n",
    "                    else:\n",
    "                        token.append('<unk>')\n",
    "            dataset[key] = {'input':feature,\n",
    "                           'text':text,\n",
    "                           'token':' '.join(token),}\n",
    "        \n",
    "\n",
    "    \n",
    "print(len(dev_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\" Access dictionary keys like attribute \n",
    "        https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "opts = AttrDict()\n",
    "\n",
    "# Configure models\n",
    "\n",
    "opts.ntoken = len(token2id)\n",
    "opts.feature = 83\n",
    "opts.ninp = 256\n",
    "opts.nhead = 4\n",
    "opts.nhid = 2048\n",
    "opts.nlayers_enc = 12\n",
    "opts.nlayers_dec = 6\n",
    "\n",
    "opts.ce_weight = 0.7\n",
    "opts.ctc_weight = 0.3\n",
    "\n",
    "\n",
    "opts.emb_size = 650\n",
    "opts.hidden_size = 650\n",
    "opts.num_layers = 2\n",
    "opts.tie_weight = True\n",
    "\n",
    "\n",
    "opts.beam_size = 10\n",
    "opts.lm_weight = 0.4\n",
    "opts.temperature = 1.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure optimization\n",
    "opts.learning_rate = 5e-5\n",
    "\n",
    "opts.dropout_rate = 0.1\n",
    "\n",
    "opts.batch_size = 64\n",
    "opts.num_workers = int(opts.batch_size / 8) if int(opts.batch_size / 8) < 16 else 16\n",
    "print(opts.num_workers)\n",
    "# Configure training\n",
    "opts.max_seq_len = 512\n",
    "opts.num_epochs = 300\n",
    "# opts.warmup_steps = 4000\n",
    "# opts.gradient_accumulation = 20\n",
    "\n",
    "# opts.load_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.names = []\n",
    "        self.features = []\n",
    "        self.texts = []\n",
    "        self.tokens = []\n",
    "        \n",
    "        for name, data in dataset.items():\n",
    "            feature = data['input']\n",
    "            text = data['text']\n",
    "            token = data['token'].split(' ')\n",
    "            token = ['<sos>'] + token + ['<eos>']\n",
    "            \n",
    "            self.names.append(name)\n",
    "            self.features.append(feature)\n",
    "            self.texts.append(text)\n",
    "            self.tokens.append(token)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        feature = self.features[index]\n",
    "        text = self.texts[index]\n",
    "        token = self.tokens[index]\n",
    "        token_id = self.tokens2ids(token, token2id)\n",
    "        \n",
    "        return name, feature, text, token, token_id\n",
    "    \n",
    "    def tokens2ids(self, tokens, token2id):\n",
    "        token_id = [token2id[token] if token in token2id else token2id['<unk>'] for token in tokens]\n",
    "    \n",
    "        return token_id\n",
    "    \n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq)-1 for seq in seqs]\n",
    "        input_seqs = torch.zeros(len(seqs), max(lens)).long().fill_(token2id['<pad>'])\n",
    "        target_seqs = torch.zeros(len(seqs), max(lens)).long().fill_(token2id['<pad>'])\n",
    "#         input_seqs_mask = input_seqs.float().masked_fill(input_seqs.float()==0, float('-inf'))\n",
    "#         input_seqs_mask = input_seqs_mask.masked_fill(input_seqs_mask!=float('-inf'), 0)\n",
    "\n",
    "        for i, seq in enumerate(seqs):\n",
    "            input_seqs[i, :len(seq)-1] = torch.LongTensor(seq[:-1])\n",
    "            target_seqs[i, :len(seq)-1] = torch.LongTensor(seq[1:])\n",
    "            \n",
    "        input_seqs_mask = ~input_seqs.bool()\n",
    "        \n",
    "        return input_seqs, input_seqs_mask, target_seqs, lens\n",
    "    \n",
    "    def _pad_features(features):\n",
    "        flens = [len(feature) for feature in features]\n",
    "        input_features = torch.zeros(len(features), max(flens), opts.feature)\n",
    "        for i, feature in enumerate(features):\n",
    "            input_features[i, :len(feature)] = feature\n",
    "            \n",
    "        return input_features\n",
    "    \n",
    "    def _generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "        \n",
    "    \n",
    "    name, feature, text, token, token_id = zip(*data)\n",
    "    \n",
    "    input_seqs, input_seqs_pad_mask, target_seqs, lens = _pad_sequences(token_id)\n",
    "    \n",
    "    input_features = _pad_features(feature)\n",
    "    \n",
    "    input_seqs_mask = _generate_square_subsequent_mask(input_seqs.size(1))\n",
    "    input_seqs_mask = input_seqs_mask.repeat(input_seqs.size(0), 1, 1) #為了給dataparallel切 要給他一個batch維\n",
    "    \n",
    "    lens = torch.LongTensor(lens)\n",
    "    \n",
    "    return name, input_features, text, token, input_seqs, input_seqs_mask, input_seqs_pad_mask, target_seqs, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset(train_data)\n",
    "dev_dataset = Dataset(dev_data)\n",
    "test_dataset = Dataset(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20200908)\n",
    "torch.manual_seed(20200908)\n",
    "torch.cuda.manual_seed_all(20200908)\n",
    "\n",
    "# train_iter = DataLoader(dataset=train_dataset,\n",
    "#                         batch_size=opts.batch_size,\n",
    "#                         shuffle=True,\n",
    "#                         num_workers=opts.num_workers,\n",
    "#                         collate_fn=collate_fn)\n",
    "\n",
    "dev_iter = DataLoader(dataset=dev_dataset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        num_workers=1,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "test_iter = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=1,\n",
    "                        shuffle=False,\n",
    "                        num_workers=1,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size, hidden_size, num_layers, tie_weight=False, dropout=0.2):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_size, \n",
    "                            hidden_size, \n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True, \n",
    "                            bidirectional=False, \n",
    "                            dropout=0.1)\n",
    "\n",
    "        self.h2w = nn.Linear(emb_size, vocab_size, bias=True)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if tie_weight:\n",
    "            self.h2w.weight = self.embedding.weight\n",
    "            \n",
    "        \n",
    "    def forward(self, input_seqs, seqs_len):     \n",
    "        \n",
    "        batch_size = input_seqs.size(0)\n",
    "        \n",
    "        emb = self.embedding(input_seqs)\n",
    "        \n",
    "        packed_output = pack_padded_sequence(emb, seqs_len, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_output, c = self.lstm(packed_output)\n",
    "\n",
    "        hidden_outputs, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        hidden_outputs = self.tanh(hidden_outputs)\n",
    "\n",
    "        outputs = self.h2w(hidden_outputs)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, Any\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        \n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        return src\n",
    "\n",
    "\n",
    "class TransformerDecoderLayer(Module):\n",
    "    r\"\"\"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = decoder_layer(tgt, memory)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.multihead_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.norm3 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "        self.dropout3 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        tgt2 = self.norm1(tgt)        \n",
    "        tgt2 = self.self_attn(tgt2, tgt2, tgt2, attn_mask=tgt_mask,\n",
    "                              key_padding_mask=tgt_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm2(tgt)\n",
    "        tgt2 = self.multihead_attn(tgt2, memory, memory, attn_mask=memory_mask,\n",
    "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        \n",
    "        tgt2 = self.norm3(tgt)\n",
    "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt2))))\n",
    "        tgt = tgt + self.dropout3(tgt2)\n",
    "        \n",
    "        return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dSubsampling(torch.nn.Module):\n",
    "    \"\"\"Convolutional 2D subsampling (to 1/4 length).\n",
    "    :param int idim: input dim\n",
    "    :param int odim: output dim\n",
    "    :param flaot dropout_rate: dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, odim, dropout_rate=0.5):\n",
    "        \"\"\"Construct an Conv2dSubsampling object.\"\"\"\n",
    "        super(Conv2dSubsampling, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, odim, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(odim, odim, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),\n",
    "#             PositionalEncoding(odim, dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"Subsample x.\n",
    "        :param torch.Tensor x: input tensor\n",
    "        :param torch.Tensor x_mask: input mask\n",
    "        :return: subsampled x and mask\n",
    "        :rtype Tuple[torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
    "        x = self.conv(x)\n",
    "        b, c, t, f = x.size()\n",
    "        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
    "        if x_mask is None:\n",
    "            return x, None\n",
    "        return x, x_mask[:, :, :-2:2][:, :, :-2:2]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, idim, dropout=0.5):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        \n",
    "        self.embedding = Conv2dSubsampling(idim, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.layer_norm = LayerNorm(ninp)\n",
    "        \n",
    "        self.ninp = ninp\n",
    "#         self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        # src [b, t, f]        \n",
    "        \n",
    "        src, self.src_mask = self.embedding(src, self.src_mask)\n",
    "\n",
    "        src = src.permute(1, 0, 2) # [t, b, f]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src=src, \n",
    "                                          mask=self.src_mask,\n",
    "                                         )\n",
    "#         output = self.decoder(output)\n",
    "        output = self.layer_norm(output)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(DecoderModel, self).__init__()\n",
    "#         from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.tgt_mask = None\n",
    "        self.memory_mask = None\n",
    "        \n",
    "        self.embedding = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.layer_norm = LayerNorm(ninp)\n",
    "        \n",
    "        self.ninp = ninp\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, tgt, tgt_mask, tgt_key_padding_mask, memory):\n",
    "        \n",
    "        # tgt [b, t], tgt_key_padding_mask [b, t], memory [t, b, f]\n",
    "        \n",
    "        tgt = tgt.permute(1, 0) # [t, b]\n",
    "        \n",
    "#         if self.tgt_mask is None or self.tgt_mask.size(0) != len(tgt):\n",
    "#             device = tgt.device\n",
    "#             mask = self._generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "#             self.tgt_mask = mask\n",
    "\n",
    "        # The reason we increase the embedding values before the addition \n",
    "        # is to make the positional encoding relatively smaller. \n",
    "        # This means the original meaning \n",
    "        # in the embedding vector won’t be lost when we add them together.\n",
    "        # maybe use learned position embedding will not need to do this?  \n",
    "        \n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.ninp)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        tgt_mask = tgt_mask[0] ##為了dataparallel給的batch維把它去掉\n",
    "        \n",
    "        output = self.transformer_decoder(tgt, memory, \n",
    "                                          tgt_mask=tgt_mask, \n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask, \n",
    "                                          memory_mask=self.memory_mask, \n",
    "                                          memory_key_padding_mask=None,\n",
    "                                          )\n",
    "        \n",
    "        output = self.layer_norm(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers_enc, nlayers_dec, idim, dropout=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = EncoderModel(ntoken, ninp, nhead, nhid, nlayers_enc, idim, dropout)\n",
    "        self.ctc_classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.decoder = DecoderModel(ntoken, ninp, nhead, nhid, nlayers_dec, dropout)\n",
    "        self.classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask, tgt_key_padding_mask):\n",
    "        \n",
    "        memory = self.encoder(src)\n",
    "        \n",
    "        ctc_output = self.ctc_classifier(memory.permute(1, 0, 2))\n",
    "        \n",
    "        decoder_output = self.decoder(tgt, tgt_mask, tgt_key_padding_mask, memory)\n",
    "        \n",
    "        output = self.classifier(decoder_output.permute(1, 0, 2))\n",
    "\n",
    "        # return 一定要batch first 不然dataparallel會concat錯維\n",
    "        return ctc_output, output, memory.permute(1, 0, 2), decoder_output.permute(1, 0, 2)\n",
    "#         return memory, ctc_output, decoder_output\n",
    "\n",
    "\n",
    "class Test_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers_enc, nlayers_dec, idim, dropout=0.5):\n",
    "        super(Test_Model, self).__init__()\n",
    "        \n",
    "        self.encoder = EncoderModel(ntoken, ninp, nhead, nhid, nlayers_enc, idim, dropout)\n",
    "        self.ctc_classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.decoder = DecoderModel(ntoken, ninp, nhead, nhid, nlayers_dec, dropout)\n",
    "        self.classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.encoder.eval()\n",
    "        self.ctc_classifier.eval()\n",
    "        self.decoder.eval()\n",
    "        self.classifier.eval()\n",
    "        \n",
    "    def forward(self, src, len_limit):\n",
    "        \n",
    "        device = src.device\n",
    "        memory = self.encoder(src)\n",
    "        \n",
    "        nbest = []\n",
    "        beams = []\n",
    "        beams.append([[token2id['<sos>']], 0])\n",
    "\n",
    "        for _ in range(2*len_limit):\n",
    "\n",
    "            results = []\n",
    "\n",
    "            for beam in beams:\n",
    "\n",
    "                input_idxs = beam[0]\n",
    "\n",
    "                input_seqs = torch.LongTensor([input_idxs])\n",
    "        #         input_seqs = input_seqs.unsqueeze(0)\n",
    "                input_seqs = input_seqs.to(device)\n",
    "\n",
    "                decoder_output = self.decoder(input_seqs, None, None, memory)\n",
    "                output = self.classifier(decoder_output.permute(1, 0, 2)[:, -1])\n",
    "                \n",
    "                output[:, token2id['<blank>']] = float('-inf')\n",
    "                output[:, token2id['<pad>']] = float('-inf')\n",
    "                output = output.log_softmax(dim=1)\n",
    "                \n",
    "                probs, idxs = output.topk(k=opts.beam_size, dim=1)\n",
    "\n",
    "                for prob, idx in zip(probs.squeeze(0), idxs.squeeze(0)):\n",
    "\n",
    "                    generate_idxs = input_idxs + [idx.item()]\n",
    "                    accumulate_prob = beam[1] + prob.item()\n",
    "\n",
    "                    results.append([generate_idxs, accumulate_prob])\n",
    "\n",
    "\n",
    "            results.sort(key=lambda x:x[1])\n",
    "            results = results[::-1]\n",
    "            results = results[:opts.beam_size]\n",
    "\n",
    "            beams = []\n",
    "\n",
    "            for result in results:\n",
    "                if result[0][-1] == token2id['<eos>']:\n",
    "                    nbest.append(result)\n",
    "                else:\n",
    "                    beams.append(result)\n",
    "                    \n",
    "            if len(beams) == 0:\n",
    "                break\n",
    "\n",
    "        return nbest, beams\n",
    "\n",
    "    \n",
    "class Test_Model_pal(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers_enc, nlayers_dec, idim, \\\n",
    "                 emb_size, hidden_size, num_layers, tie_weight, dropout=0.5):\n",
    "        \n",
    "        super(Test_Model_pal, self).__init__()\n",
    "        \n",
    "        self.encoder = EncoderModel(ntoken, ninp, nhead, nhid, nlayers_enc, idim, dropout)\n",
    "        self.ctc_classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.decoder = DecoderModel(ntoken, ninp, nhead, nhid, nlayers_dec, dropout)\n",
    "        self.classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.rnnlm = RNNLM(ntoken, emb_size, hidden_size, num_layers, tie_weight, dropout)\n",
    "        \n",
    "        self.encoder.eval()\n",
    "        self.ctc_classifier.eval()\n",
    "        self.decoder.eval()\n",
    "        self.classifier.eval()\n",
    "        self.rnnlm.eval()\n",
    "        \n",
    "    def forward(self, src, len_limit):\n",
    "        \n",
    "        device = src.device\n",
    "        memory = self.encoder(src)\n",
    "        \n",
    "        nbest = []\n",
    "        beams = []\n",
    "        beams.append([[token2id['<sos>']], 0])\n",
    "\n",
    "        for _ in range(int(src.size(1) * 0.1)):\n",
    "            \n",
    "            results = []\n",
    "    \n",
    "            input_inxss = []\n",
    "            for beam in beams:\n",
    "                input_inxss.append(beam[0])\n",
    "\n",
    "            lens = [len(input_idxs) for input_idxs in input_inxss]\n",
    "\n",
    "            input_seqs = torch.LongTensor(len(input_inxss), max(lens)).fill_(token2id['<pad>']).to(device)\n",
    "            seqs_len = torch.LongTensor(lens).to(device)\n",
    "\n",
    "            for i, input_idxs in enumerate(input_inxss):\n",
    "                input_seqs[i, :len(input_idxs)] = torch.LongTensor(input_idxs)\n",
    "\n",
    "            input_seqs_mask = input_seqs == token2id['<pad>']\n",
    "\n",
    "            this_memory = memory.repeat(1, input_seqs.size(0), 1)\n",
    "            \n",
    "            input_seqs = input_seqs.to(device)\n",
    "            input_seqs_mask = input_seqs_mask.to(device)\n",
    "\n",
    "            decoder_output = self.decoder(input_seqs, None, input_seqs_mask, this_memory)\n",
    "\n",
    "            output = self.classifier(decoder_output.permute(1, 0, 2)[:, -1, :])\n",
    "            \n",
    "            lm_output = self.rnnlm(input_seqs, seqs_len)\n",
    "\n",
    "            lm_output = lm_output[:, -1, :]\n",
    "            \n",
    "            lm_output = lm_output / opts.temperature\n",
    "\n",
    "            output[:, token2id['<pad>']] = float('-inf')\n",
    "            output[:, token2id['<blank>']] = float('-inf')\n",
    "            lm_output[:, token2id['<pad>']] = float('-inf')\n",
    "            lm_output[:, token2id['<blank>']] = float('-inf')\n",
    "\n",
    "            output = output.log_softmax(dim=1)\n",
    "            lm_output = lm_output.log_softmax(dim=1)\n",
    "\n",
    "            total_output = output + (opts.lm_weight*lm_output)\n",
    "\n",
    "#             total_output = output\n",
    "\n",
    "#             total_output = total_output.log()\n",
    "\n",
    "            probs, idxs = total_output.topk(k=opts.beam_size, dim=1)\n",
    "\n",
    "            for i, (batch_probs, batch_idxs) in enumerate(zip(probs, idxs)):\n",
    "                for j, (prob, idx) in enumerate(zip(batch_probs, batch_idxs)):\n",
    "\n",
    "                    generate_idxs = beams[i][0] + [idx.item()]\n",
    "                    accumulate_prob = beams[i][1] + prob.item()\n",
    "\n",
    "                    results.append([generate_idxs, accumulate_prob])\n",
    "\n",
    "            results.sort(key=lambda x:x[1])\n",
    "            results = results[::-1]\n",
    "            results = results[:opts.beam_size]\n",
    "\n",
    "            beams = []\n",
    "\n",
    "            for result in results:\n",
    "                if result[0][-1] == token2id['<eos>']:\n",
    "                    nbest.append(result)\n",
    "                else:\n",
    "                    beams.append(result)\n",
    "                    \n",
    "            if len(beams) == 0:\n",
    "                break\n",
    "\n",
    "        return nbest, beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = Test_Model_pal(opts.ntoken, opts.ninp, opts.nhead, opts.nhid, \\\n",
    "                         opts.nlayers_enc, opts.nlayers_dec, opts.feature, \\\n",
    "                        opts.emb_size, opts.hidden_size, opts.num_layers, \\\n",
    "                         opts.tie_weight, opts.dropout_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2e_ckpt = torch.load(\"exp/**myaishell_ce0.7_ctc0.3_2020-09-15 17:15:39/epoch_16.ckpt\", map_location='cpu')\n",
    "e2e_parms = e2e_ckpt['net']\n",
    "\n",
    "lm_ckpt = torch.load(\"exp/RNNLM_2layers_650hidden_2020-09-16 08:51:44/epoch_26.ckpt\", map_location='cpu')\n",
    "lm_parms = lm_ckpt['net']\n",
    "\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "    \n",
    "new_e2e_parms = OrderedDict()\n",
    "for k, v in e2e_parms.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_e2e_parms[name] = v\n",
    "\n",
    "new_lm_parms = OrderedDict()\n",
    "for k, v in lm_parms.items():\n",
    "    name = k[7:] # remove `module.`\n",
    "    new_lm_parms[name] = v\n",
    "\n",
    "print(t_model.load_state_dict(new_e2e_parms, strict=False))\n",
    "print(t_model.rnnlm.load_state_dict(new_lm_parms, strict=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class error_stats:\n",
    "    def __init__(self):\n",
    "        self.ins_num = 0 \n",
    "        self.del_num = 0\n",
    "        self.sub_num = 0\n",
    "        self.total_cost = 0\n",
    "        \n",
    "# ref=['聽', '說', '馬', '上', '就', '要', '放', '假', '了']\n",
    "# hyp=['你', '聽', '說', '要', '放', '假', '了']\n",
    "\n",
    "def wer(ref, hyp):\n",
    "    N = len(ref)\n",
    "    e = []\n",
    "    for i in range(len(ref)+1):\n",
    "        e.append(error_stats())\n",
    "    cur_e = []\n",
    "    for i in range(len(ref)+1):\n",
    "        cur_e.append(error_stats)\n",
    "\n",
    "    for i in range(len(e)):\n",
    "        e[i].ins_num = 0\n",
    "        e[i].sub_num = 0\n",
    "        e[i].del_num = i\n",
    "        e[i].total_cost = i\n",
    "\n",
    "    for hyp_index in range(1, len(hyp)+1):\n",
    "        cur_e[0] = copy.copy(e[0])\n",
    "\n",
    "        cur_e[0].ins_num+=1\n",
    "        cur_e[0].total_cost+=1\n",
    "        for ref_index in range(1, len(ref)+1):\n",
    "            ins_err = e[ref_index].total_cost + 1\n",
    "            #print(cur_e[ref_index-1].total_cost)\n",
    "            del_err = cur_e[ref_index-1].total_cost + 1\n",
    "            sub_err = e[ref_index-1].total_cost\n",
    "            #print(ins_err, del_err, sub_err)\n",
    "            #print(e[0].total_cost)\n",
    "            if hyp[hyp_index-1] != ref[ref_index-1]:\n",
    "                sub_err+=1\n",
    "            #print(ins_err, del_err, sub_err)\n",
    "            if sub_err < ins_err and sub_err < del_err:\n",
    "                cur_e[ref_index] = copy.copy(e[ref_index-1])\n",
    "                \n",
    "                if hyp[hyp_index-1] != ref[ref_index-1]:\n",
    "                    cur_e[ref_index].sub_num+=1\n",
    "                cur_e[ref_index].total_cost = sub_err\n",
    "            elif del_err < ins_err:\n",
    "                cur_e[ref_index] = copy.copy(cur_e[ref_index-1])\n",
    "\n",
    "                cur_e[ref_index].total_cost = del_err\n",
    "                cur_e[ref_index].del_num+=1\n",
    "            else:\n",
    "                cur_e[ref_index] = copy.copy(e[ref_index])\n",
    "\n",
    "                cur_e[ref_index].total_cost = ins_err\n",
    "                cur_e[ref_index].ins_num+=1\n",
    "        e = cur_e.copy()\n",
    "\n",
    "    ref_index = len(e)-1\n",
    "    Ins = e[ref_index].ins_num\n",
    "    Del = e[ref_index].del_num\n",
    "    Sub = e[ref_index].sub_num\n",
    "    Cost = e[ref_index].total_cost\n",
    "    #print(Ins, Del, Sub, Cost)\n",
    "    return Ins, Del, Sub, Cost, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# USE_CUDA = False\n",
    "\n",
    "if USE_CUDA:\n",
    "    t_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = notebook.tqdm(total = len(test_iter))\n",
    "\n",
    "nbests = []\n",
    "refs = []\n",
    "\n",
    "\n",
    "for batch in test_iter:\n",
    "    \n",
    "    name, input_features, text, token, input_seqs, input_seqs_mask, input_seqs_pad_mask, target_seqs, lens = batch\n",
    "    \n",
    "    t_model.eval()\n",
    "    \n",
    "    len_limit = int(input_features.size(1) * 0.1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_features = input_features.cuda()\n",
    "    \n",
    "    \n",
    "    nbest, beams = t_model(input_features, len_limit)\n",
    "    \n",
    "    if len(nbest) == 0:\n",
    "        nbest = beams\n",
    "        \n",
    "    nbest.sort(key=lambda x:x[1])\n",
    "    nbest = nbest[::-1]\n",
    "        \n",
    "    nbests.append(nbest)\n",
    "    refs.append(token)\n",
    "    \n",
    "    pbar.update(1)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join([id2token[token] for token in nbests[10][0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(refs), len(nbests))\n",
    "\n",
    "totalN = 0\n",
    "totalIns = 0\n",
    "totalDel = 0\n",
    "totalSub = 0\n",
    "\n",
    "\n",
    "\n",
    "for ref, nbest in zip(refs, nbests):\n",
    "    \n",
    "    nbest.sort(key=lambda x:x[1])\n",
    "    nbest = nbest[::-1]\n",
    "    \n",
    "    hyp = [id2token[token] for token in nbest[0][0]]\n",
    "    \n",
    "    if hyp[0] == '<sos>':\n",
    "        hyp = hyp[1:]\n",
    "    if hyp[-1] == '<eos>':\n",
    "        hyp = hyp[:-1]\n",
    "        \n",
    "#     print(' '.join(hyp))\n",
    "\n",
    "    ref = ref[0]  \n",
    "    ref = ref[1:-1]\n",
    "    \n",
    "    Ins, Del, Sub, Cost, N = wer(ref, hyp)\n",
    "    totalN += N\n",
    "    totalIns += Ins\n",
    "    totalDel += Del\n",
    "    totalSub += Sub\n",
    "    \n",
    "    \n",
    "print(totalN, totalIns, totalDel, totalSub)\n",
    "\n",
    "print('wer : {}'.format((totalIns+totalDel+totalSub)/totalN*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
