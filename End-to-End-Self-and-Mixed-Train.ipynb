{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from kaldiio import ReadHelper\n",
    "import json\n",
    "import os \n",
    "import time\n",
    "from tqdm import notebook\n",
    "import copy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "espnet_path = Path('espnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "egs_path = Path('egs/aishell/asr1')\n",
    "exp_dir = espnet_path / egs_path / 'exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_path = 'dump'\n",
    "train_path = 'train_sp'\n",
    "dev_path = 'dev'\n",
    "test_path = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4231\n"
     ]
    }
   ],
   "source": [
    "vocab_file = espnet_path / egs_path / 'data/lang_1char/train_sp_units.txt'\n",
    "vocab = {}\n",
    "with open(vocab_file) as fp:\n",
    "    for line in fp:\n",
    "        word, idx = line.strip().split()\n",
    "        vocab[word] = idx\n",
    "        \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_id = 4\n",
    "token2id = {'<blank>':0, '<pad>':1, '<sos>':2, '<eos>':3}\n",
    "for word, _ in vocab.items():\n",
    "    if word not in token2id:\n",
    "        token2id[word] = new_id\n",
    "        new_id += 1\n",
    "\n",
    "id2token = {v:k for k, v in token2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<blank>': 0,\n",
       " '<pad>': 1,\n",
       " '<sos>': 2,\n",
       " '<eos>': 3,\n",
       " '<unk>': 4,\n",
       " '一': 5,\n",
       " '丁': 6,\n",
       " '七': 7,\n",
       " '万': 8,\n",
       " '丈': 9,\n",
       " '三': 10,\n",
       " '上': 11,\n",
       " '下': 12,\n",
       " '不': 13,\n",
       " '与': 14,\n",
       " '丐': 15,\n",
       " '丑': 16,\n",
       " '专': 17,\n",
       " '且': 18,\n",
       " '世': 19,\n",
       " '丘': 20,\n",
       " '丙': 21,\n",
       " '业': 22,\n",
       " '丛': 23,\n",
       " '东': 24,\n",
       " '丝': 25,\n",
       " '丞': 26,\n",
       " '丢': 27,\n",
       " '两': 28,\n",
       " '严': 29,\n",
       " '丧': 30,\n",
       " '个': 31,\n",
       " '丫': 32,\n",
       " '中': 33,\n",
       " '丰': 34,\n",
       " '串': 35,\n",
       " '临': 36,\n",
       " '丸': 37,\n",
       " '丹': 38,\n",
       " '为': 39,\n",
       " '主': 40,\n",
       " '丽': 41,\n",
       " '举': 42,\n",
       " '乃': 43,\n",
       " '久': 44,\n",
       " '么': 45,\n",
       " '义': 46,\n",
       " '之': 47,\n",
       " '乌': 48,\n",
       " '乍': 49,\n",
       " '乎': 50,\n",
       " '乏': 51,\n",
       " '乐': 52,\n",
       " '乒': 53,\n",
       " '乓': 54,\n",
       " '乔': 55,\n",
       " '乖': 56,\n",
       " '乘': 57,\n",
       " '乙': 58,\n",
       " '九': 59,\n",
       " '乞': 60,\n",
       " '也': 61,\n",
       " '习': 62,\n",
       " '乡': 63,\n",
       " '书': 64,\n",
       " '买': 65,\n",
       " '乱': 66,\n",
       " '乳': 67,\n",
       " '乾': 68,\n",
       " '了': 69,\n",
       " '予': 70,\n",
       " '争': 71,\n",
       " '事': 72,\n",
       " '二': 73,\n",
       " '于': 74,\n",
       " '亏': 75,\n",
       " '云': 76,\n",
       " '互': 77,\n",
       " '五': 78,\n",
       " '井': 79,\n",
       " '亚': 80,\n",
       " '些': 81,\n",
       " '亟': 82,\n",
       " '亡': 83,\n",
       " '亢': 84,\n",
       " '交': 85,\n",
       " '亥': 86,\n",
       " '亦': 87,\n",
       " '产': 88,\n",
       " '亨': 89,\n",
       " '亩': 90,\n",
       " '享': 91,\n",
       " '京': 92,\n",
       " '亭': 93,\n",
       " '亮': 94,\n",
       " '亲': 95,\n",
       " '亳': 96,\n",
       " '亵': 97,\n",
       " '人': 98,\n",
       " '亿': 99,\n",
       " '什': 100,\n",
       " '仁': 101,\n",
       " '仄': 102,\n",
       " '仅': 103,\n",
       " '仇': 104,\n",
       " '今': 105,\n",
       " '介': 106,\n",
       " '仍': 107,\n",
       " '从': 108,\n",
       " '仑': 109,\n",
       " '仓': 110,\n",
       " '仔': 111,\n",
       " '仕': 112,\n",
       " '他': 113,\n",
       " '仗': 114,\n",
       " '付': 115,\n",
       " '仙': 116,\n",
       " '仡': 117,\n",
       " '代': 118,\n",
       " '令': 119,\n",
       " '以': 120,\n",
       " '仨': 121,\n",
       " '仪': 122,\n",
       " '们': 123,\n",
       " '仰': 124,\n",
       " '仲': 125,\n",
       " '件': 126,\n",
       " '价': 127,\n",
       " '任': 128,\n",
       " '份': 129,\n",
       " '仿': 130,\n",
       " '企': 131,\n",
       " '伉': 132,\n",
       " '伊': 133,\n",
       " '伍': 134,\n",
       " '伎': 135,\n",
       " '伏': 136,\n",
       " '伐': 137,\n",
       " '休': 138,\n",
       " '众': 139,\n",
       " '优': 140,\n",
       " '伙': 141,\n",
       " '会': 142,\n",
       " '伞': 143,\n",
       " '伟': 144,\n",
       " '传': 145,\n",
       " '伢': 146,\n",
       " '伤': 147,\n",
       " '伦': 148,\n",
       " '伪': 149,\n",
       " '伯': 150,\n",
       " '估': 151,\n",
       " '伴': 152,\n",
       " '伶': 153,\n",
       " '伸': 154,\n",
       " '伺': 155,\n",
       " '似': 156,\n",
       " '伽': 157,\n",
       " '佃': 158,\n",
       " '但': 159,\n",
       " '位': 160,\n",
       " '低': 161,\n",
       " '住': 162,\n",
       " '佐': 163,\n",
       " '佑': 164,\n",
       " '体': 165,\n",
       " '何': 166,\n",
       " '佘': 167,\n",
       " '余': 168,\n",
       " '佛': 169,\n",
       " '作': 170,\n",
       " '佟': 171,\n",
       " '你': 172,\n",
       " '佣': 173,\n",
       " '佩': 174,\n",
       " '佬': 175,\n",
       " '佳': 176,\n",
       " '佶': 177,\n",
       " '佼': 178,\n",
       " '使': 179,\n",
       " '侃': 180,\n",
       " '侄': 181,\n",
       " '侈': 182,\n",
       " '例': 183,\n",
       " '侍': 184,\n",
       " '侑': 185,\n",
       " '侗': 186,\n",
       " '供': 187,\n",
       " '依': 188,\n",
       " '侠': 189,\n",
       " '侣': 190,\n",
       " '侥': 191,\n",
       " '侦': 192,\n",
       " '侧': 193,\n",
       " '侨': 194,\n",
       " '侬': 195,\n",
       " '侮': 196,\n",
       " '侯': 197,\n",
       " '侵': 198,\n",
       " '便': 199,\n",
       " '促': 200,\n",
       " '俄': 201,\n",
       " '俊': 202,\n",
       " '俏': 203,\n",
       " '俐': 204,\n",
       " '俗': 205,\n",
       " '俘': 206,\n",
       " '俚': 207,\n",
       " '保': 208,\n",
       " '俞': 209,\n",
       " '信': 210,\n",
       " '俨': 211,\n",
       " '俩': 212,\n",
       " '俪': 213,\n",
       " '俭': 214,\n",
       " '修': 215,\n",
       " '俯': 216,\n",
       " '俱': 217,\n",
       " '俸': 218,\n",
       " '俺': 219,\n",
       " '俾': 220,\n",
       " '倍': 221,\n",
       " '倒': 222,\n",
       " '倘': 223,\n",
       " '候': 224,\n",
       " '倚': 225,\n",
       " '倜': 226,\n",
       " '借': 227,\n",
       " '倡': 228,\n",
       " '倦': 229,\n",
       " '倩': 230,\n",
       " '倪': 231,\n",
       " '债': 232,\n",
       " '值': 233,\n",
       " '倾': 234,\n",
       " '假': 235,\n",
       " '偏': 236,\n",
       " '做': 237,\n",
       " '停': 238,\n",
       " '健': 239,\n",
       " '偶': 240,\n",
       " '偷': 241,\n",
       " '偿': 242,\n",
       " '傅': 243,\n",
       " '傍': 244,\n",
       " '傥': 245,\n",
       " '储': 246,\n",
       " '催': 247,\n",
       " '傲': 248,\n",
       " '傻': 249,\n",
       " '像': 250,\n",
       " '僚': 251,\n",
       " '僧': 252,\n",
       " '僮': 253,\n",
       " '僵': 254,\n",
       " '僻': 255,\n",
       " '儒': 256,\n",
       " '儿': 257,\n",
       " '兀': 258,\n",
       " '允': 259,\n",
       " '元': 260,\n",
       " '兄': 261,\n",
       " '充': 262,\n",
       " '兆': 263,\n",
       " '先': 264,\n",
       " '光': 265,\n",
       " '克': 266,\n",
       " '免': 267,\n",
       " '兑': 268,\n",
       " '兔': 269,\n",
       " '兖': 270,\n",
       " '党': 271,\n",
       " '兜': 272,\n",
       " '兢': 273,\n",
       " '入': 274,\n",
       " '全': 275,\n",
       " '八': 276,\n",
       " '公': 277,\n",
       " '六': 278,\n",
       " '兰': 279,\n",
       " '共': 280,\n",
       " '关': 281,\n",
       " '兴': 282,\n",
       " '兵': 283,\n",
       " '其': 284,\n",
       " '具': 285,\n",
       " '典': 286,\n",
       " '兹': 287,\n",
       " '养': 288,\n",
       " '兼': 289,\n",
       " '兽': 290,\n",
       " '冀': 291,\n",
       " '内': 292,\n",
       " '冈': 293,\n",
       " '冉': 294,\n",
       " '册': 295,\n",
       " '再': 296,\n",
       " '冒': 297,\n",
       " '冕': 298,\n",
       " '写': 299,\n",
       " '军': 300,\n",
       " '农': 301,\n",
       " '冠': 302,\n",
       " '冤': 303,\n",
       " '冥': 304,\n",
       " '冬': 305,\n",
       " '冯': 306,\n",
       " '冰': 307,\n",
       " '冲': 308,\n",
       " '决': 309,\n",
       " '况': 310,\n",
       " '冶': 311,\n",
       " '冷': 312,\n",
       " '冻': 313,\n",
       " '净': 314,\n",
       " '凄': 315,\n",
       " '准': 316,\n",
       " '凇': 317,\n",
       " '凉': 318,\n",
       " '凋': 319,\n",
       " '凌': 320,\n",
       " '减': 321,\n",
       " '凑': 322,\n",
       " '凝': 323,\n",
       " '几': 324,\n",
       " '凡': 325,\n",
       " '凤': 326,\n",
       " '凭': 327,\n",
       " '凯': 328,\n",
       " '凰': 329,\n",
       " '凳': 330,\n",
       " '凶': 331,\n",
       " '凸': 332,\n",
       " '凹': 333,\n",
       " '出': 334,\n",
       " '击': 335,\n",
       " '函': 336,\n",
       " '凿': 337,\n",
       " '刀': 338,\n",
       " '刁': 339,\n",
       " '刃': 340,\n",
       " '分': 341,\n",
       " '切': 342,\n",
       " '刊': 343,\n",
       " '刑': 344,\n",
       " '划': 345,\n",
       " '列': 346,\n",
       " '刘': 347,\n",
       " '则': 348,\n",
       " '刚': 349,\n",
       " '创': 350,\n",
       " '初': 351,\n",
       " '删': 352,\n",
       " '判': 353,\n",
       " '刨': 354,\n",
       " '利': 355,\n",
       " '别': 356,\n",
       " '刮': 357,\n",
       " '到': 358,\n",
       " '制': 359,\n",
       " '刷': 360,\n",
       " '券': 361,\n",
       " '刹': 362,\n",
       " '刺': 363,\n",
       " '刻': 364,\n",
       " '剁': 365,\n",
       " '剂': 366,\n",
       " '剃': 367,\n",
       " '削': 368,\n",
       " '前': 369,\n",
       " '剐': 370,\n",
       " '剑': 371,\n",
       " '剔': 372,\n",
       " '剖': 373,\n",
       " '剥': 374,\n",
       " '剧': 375,\n",
       " '剩': 376,\n",
       " '剪': 377,\n",
       " '副': 378,\n",
       " '割': 379,\n",
       " '剽': 380,\n",
       " '剿': 381,\n",
       " '劈': 382,\n",
       " '力': 383,\n",
       " '劝': 384,\n",
       " '办': 385,\n",
       " '功': 386,\n",
       " '加': 387,\n",
       " '务': 388,\n",
       " '劣': 389,\n",
       " '动': 390,\n",
       " '助': 391,\n",
       " '努': 392,\n",
       " '劫': 393,\n",
       " '励': 394,\n",
       " '劲': 395,\n",
       " '劳': 396,\n",
       " '劵': 397,\n",
       " '势': 398,\n",
       " '勃': 399,\n",
       " '勇': 400,\n",
       " '勉': 401,\n",
       " '勋': 402,\n",
       " '勒': 403,\n",
       " '勘': 404,\n",
       " '募': 405,\n",
       " '勤': 406,\n",
       " '勺': 407,\n",
       " '勾': 408,\n",
       " '勿': 409,\n",
       " '匀': 410,\n",
       " '包': 411,\n",
       " '匆': 412,\n",
       " '匈': 413,\n",
       " '匕': 414,\n",
       " '化': 415,\n",
       " '北': 416,\n",
       " '匙': 417,\n",
       " '匝': 418,\n",
       " '匠': 419,\n",
       " '匡': 420,\n",
       " '匣': 421,\n",
       " '匪': 422,\n",
       " '匮': 423,\n",
       " '匹': 424,\n",
       " '区': 425,\n",
       " '医': 426,\n",
       " '匾': 427,\n",
       " '匿': 428,\n",
       " '十': 429,\n",
       " '千': 430,\n",
       " '升': 431,\n",
       " '午': 432,\n",
       " '卉': 433,\n",
       " '半': 434,\n",
       " '华': 435,\n",
       " '协': 436,\n",
       " '卑': 437,\n",
       " '卒': 438,\n",
       " '卓': 439,\n",
       " '单': 440,\n",
       " '卖': 441,\n",
       " '南': 442,\n",
       " '博': 443,\n",
       " '卜': 444,\n",
       " '卞': 445,\n",
       " '占': 446,\n",
       " '卡': 447,\n",
       " '卢': 448,\n",
       " '卤': 449,\n",
       " '卦': 450,\n",
       " '卧': 451,\n",
       " '卫': 452,\n",
       " '卯': 453,\n",
       " '印': 454,\n",
       " '危': 455,\n",
       " '卲': 456,\n",
       " '即': 457,\n",
       " '却': 458,\n",
       " '卵': 459,\n",
       " '卷': 460,\n",
       " '卸': 461,\n",
       " '卿': 462,\n",
       " '厂': 463,\n",
       " '厄': 464,\n",
       " '厅': 465,\n",
       " '历': 466,\n",
       " '厉': 467,\n",
       " '压': 468,\n",
       " '厌': 469,\n",
       " '厕': 470,\n",
       " '厘': 471,\n",
       " '厚': 472,\n",
       " '原': 473,\n",
       " '厢': 474,\n",
       " '厥': 475,\n",
       " '厦': 476,\n",
       " '厨': 477,\n",
       " '厩': 478,\n",
       " '厮': 479,\n",
       " '去': 480,\n",
       " '县': 481,\n",
       " '参': 482,\n",
       " '又': 483,\n",
       " '叉': 484,\n",
       " '及': 485,\n",
       " '友': 486,\n",
       " '双': 487,\n",
       " '反': 488,\n",
       " '发': 489,\n",
       " '叔': 490,\n",
       " '取': 491,\n",
       " '受': 492,\n",
       " '变': 493,\n",
       " '叙': 494,\n",
       " '叛': 495,\n",
       " '叠': 496,\n",
       " '口': 497,\n",
       " '古': 498,\n",
       " '句': 499,\n",
       " '另': 500,\n",
       " '叨': 501,\n",
       " '叩': 502,\n",
       " '只': 503,\n",
       " '叫': 504,\n",
       " '召': 505,\n",
       " '叭': 506,\n",
       " '叮': 507,\n",
       " '可': 508,\n",
       " '台': 509,\n",
       " '叱': 510,\n",
       " '史': 511,\n",
       " '右': 512,\n",
       " '叵': 513,\n",
       " '叶': 514,\n",
       " '号': 515,\n",
       " '司': 516,\n",
       " '叹': 517,\n",
       " '叼': 518,\n",
       " '吁': 519,\n",
       " '吃': 520,\n",
       " '各': 521,\n",
       " '吆': 522,\n",
       " '合': 523,\n",
       " '吉': 524,\n",
       " '吊': 525,\n",
       " '同': 526,\n",
       " '名': 527,\n",
       " '后': 528,\n",
       " '吏': 529,\n",
       " '吐': 530,\n",
       " '向': 531,\n",
       " '吓': 532,\n",
       " '吕': 533,\n",
       " '吗': 534,\n",
       " '君': 535,\n",
       " '吝': 536,\n",
       " '吞': 537,\n",
       " '吟': 538,\n",
       " '否': 539,\n",
       " '吧': 540,\n",
       " '吨': 541,\n",
       " '吩': 542,\n",
       " '含': 543,\n",
       " '听': 544,\n",
       " '吭': 545,\n",
       " '启': 546,\n",
       " '吴': 547,\n",
       " '吵': 548,\n",
       " '吸': 549,\n",
       " '吹': 550,\n",
       " '吻': 551,\n",
       " '吼': 552,\n",
       " '吾': 553,\n",
       " '吿': 554,\n",
       " '呀': 555,\n",
       " '呃': 556,\n",
       " '呆': 557,\n",
       " '呈': 558,\n",
       " '告': 559,\n",
       " '呐': 560,\n",
       " '呕': 561,\n",
       " '呗': 562,\n",
       " '员': 563,\n",
       " '呛': 564,\n",
       " '呜': 565,\n",
       " '呢': 566,\n",
       " '呦': 567,\n",
       " '周': 568,\n",
       " '呲': 569,\n",
       " '味': 570,\n",
       " '呵': 571,\n",
       " '呼': 572,\n",
       " '命': 573,\n",
       " '咀': 574,\n",
       " '咄': 575,\n",
       " '咋': 576,\n",
       " '和': 577,\n",
       " '咎': 578,\n",
       " '咏': 579,\n",
       " '咐': 580,\n",
       " '咒': 581,\n",
       " '咔': 582,\n",
       " '咕': 583,\n",
       " '咖': 584,\n",
       " '咚': 585,\n",
       " '咣': 586,\n",
       " '咤': 587,\n",
       " '咧': 588,\n",
       " '咨': 589,\n",
       " '咪': 590,\n",
       " '咫': 591,\n",
       " '咬': 592,\n",
       " '咯': 593,\n",
       " '咱': 594,\n",
       " '咳': 595,\n",
       " '咸': 596,\n",
       " '咽': 597,\n",
       " '哀': 598,\n",
       " '品': 599,\n",
       " '哄': 600,\n",
       " '哆': 601,\n",
       " '哇': 602,\n",
       " '哈': 603,\n",
       " '哉': 604,\n",
       " '响': 605,\n",
       " '哎': 606,\n",
       " '哑': 607,\n",
       " '哒': 608,\n",
       " '哗': 609,\n",
       " '哟': 610,\n",
       " '哥': 611,\n",
       " '哦': 612,\n",
       " '哨': 613,\n",
       " '哪': 614,\n",
       " '哭': 615,\n",
       " '哲': 616,\n",
       " '哺': 617,\n",
       " '哼': 618,\n",
       " '哽': 619,\n",
       " '唁': 620,\n",
       " '唇': 621,\n",
       " '唉': 622,\n",
       " '唏': 623,\n",
       " '唐': 624,\n",
       " '唠': 625,\n",
       " '唤': 626,\n",
       " '唬': 627,\n",
       " '售': 628,\n",
       " '唯': 629,\n",
       " '唱': 630,\n",
       " '唾': 631,\n",
       " '啃': 632,\n",
       " '商': 633,\n",
       " '啊': 634,\n",
       " '啕': 635,\n",
       " '啡': 636,\n",
       " '啤': 637,\n",
       " '啥': 638,\n",
       " '啦': 639,\n",
       " '啧': 640,\n",
       " '啪': 641,\n",
       " '啬': 642,\n",
       " '啰': 643,\n",
       " '啵': 644,\n",
       " '啶': 645,\n",
       " '啸': 646,\n",
       " '啼': 647,\n",
       " '喀': 648,\n",
       " '喂': 649,\n",
       " '善': 650,\n",
       " '喆': 651,\n",
       " '喇': 652,\n",
       " '喉': 653,\n",
       " '喊': 654,\n",
       " '喔': 655,\n",
       " '喘': 656,\n",
       " '喜': 657,\n",
       " '喝': 658,\n",
       " '喧': 659,\n",
       " '喱': 660,\n",
       " '喵': 661,\n",
       " '喷': 662,\n",
       " '喻': 663,\n",
       " '喽': 664,\n",
       " '嗅': 665,\n",
       " '嗑': 666,\n",
       " '嗒': 667,\n",
       " '嗓': 668,\n",
       " '嗡': 669,\n",
       " '嗣': 670,\n",
       " '嗤': 671,\n",
       " '嗦': 672,\n",
       " '嗨': 673,\n",
       " '嗬': 674,\n",
       " '嗯': 675,\n",
       " '嗲': 676,\n",
       " '嗷': 677,\n",
       " '嗽': 678,\n",
       " '嘀': 679,\n",
       " '嘉': 680,\n",
       " '嘎': 681,\n",
       " '嘘': 682,\n",
       " '嘛': 683,\n",
       " '嘟': 684,\n",
       " '嘭': 685,\n",
       " '嘱': 686,\n",
       " '嘲': 687,\n",
       " '嘴': 688,\n",
       " '嘻': 689,\n",
       " '噎': 690,\n",
       " '器': 691,\n",
       " '噩': 692,\n",
       " '噪': 693,\n",
       " '噬': 694,\n",
       " '噱': 695,\n",
       " '噼': 696,\n",
       " '嚎': 697,\n",
       " '嚏': 698,\n",
       " '嚓': 699,\n",
       " '嚣': 700,\n",
       " '嚷': 701,\n",
       " '嚼': 702,\n",
       " '囊': 703,\n",
       " '囚': 704,\n",
       " '四': 705,\n",
       " '回': 706,\n",
       " '因': 707,\n",
       " '团': 708,\n",
       " '囤': 709,\n",
       " '囧': 710,\n",
       " '园': 711,\n",
       " '困': 712,\n",
       " '围': 713,\n",
       " '固': 714,\n",
       " '国': 715,\n",
       " '图': 716,\n",
       " '圆': 717,\n",
       " '圈': 718,\n",
       " '土': 719,\n",
       " '圣': 720,\n",
       " '在': 721,\n",
       " '圩': 722,\n",
       " '圪': 723,\n",
       " '圭': 724,\n",
       " '地': 725,\n",
       " '圳': 726,\n",
       " '场': 727,\n",
       " '圾': 728,\n",
       " '址': 729,\n",
       " '坂': 730,\n",
       " '均': 731,\n",
       " '坊': 732,\n",
       " '坍': 733,\n",
       " '坎': 734,\n",
       " '坏': 735,\n",
       " '坐': 736,\n",
       " '坑': 737,\n",
       " '块': 738,\n",
       " '坚': 739,\n",
       " '坛': 740,\n",
       " '坝': 741,\n",
       " '坞': 742,\n",
       " '坟': 743,\n",
       " '坠': 744,\n",
       " '坡': 745,\n",
       " '坤': 746,\n",
       " '坦': 747,\n",
       " '坪': 748,\n",
       " '坯': 749,\n",
       " '坷': 750,\n",
       " '垂': 751,\n",
       " '垃': 752,\n",
       " '垄': 753,\n",
       " '垅': 754,\n",
       " '型': 755,\n",
       " '垌': 756,\n",
       " '垒': 757,\n",
       " '垛': 758,\n",
       " '垢': 759,\n",
       " '垣': 760,\n",
       " '垤': 761,\n",
       " '垦': 762,\n",
       " '垫': 763,\n",
       " '垮': 764,\n",
       " '埃': 765,\n",
       " '埋': 766,\n",
       " '城': 767,\n",
       " '埔': 768,\n",
       " '埜': 769,\n",
       " '域': 770,\n",
       " '培': 771,\n",
       " '基': 772,\n",
       " '堂': 773,\n",
       " '堆': 774,\n",
       " '堕': 775,\n",
       " '堡': 776,\n",
       " '堤': 777,\n",
       " '堪': 778,\n",
       " '堰': 779,\n",
       " '堵': 780,\n",
       " '塌': 781,\n",
       " '塑': 782,\n",
       " '塔': 783,\n",
       " '塘': 784,\n",
       " '塞': 785,\n",
       " '填': 786,\n",
       " '塬': 787,\n",
       " '塾': 788,\n",
       " '境': 789,\n",
       " '墅': 790,\n",
       " '墓': 791,\n",
       " '墙': 792,\n",
       " '增': 793,\n",
       " '墟': 794,\n",
       " '墨': 795,\n",
       " '墩': 796,\n",
       " '壁': 797,\n",
       " '壑': 798,\n",
       " '壕': 799,\n",
       " '壤': 800,\n",
       " '士': 801,\n",
       " '壮': 802,\n",
       " '声': 803,\n",
       " '壳': 804,\n",
       " '壶': 805,\n",
       " '壹': 806,\n",
       " '处': 807,\n",
       " '备': 808,\n",
       " '复': 809,\n",
       " '夏': 810,\n",
       " '夕': 811,\n",
       " '外': 812,\n",
       " '夙': 813,\n",
       " '多': 814,\n",
       " '夜': 815,\n",
       " '够': 816,\n",
       " '大': 817,\n",
       " '天': 818,\n",
       " '太': 819,\n",
       " '夫': 820,\n",
       " '夭': 821,\n",
       " '央': 822,\n",
       " '夯': 823,\n",
       " '失': 824,\n",
       " '头': 825,\n",
       " '夷': 826,\n",
       " '夸': 827,\n",
       " '夹': 828,\n",
       " '夺': 829,\n",
       " '奂': 830,\n",
       " '奇': 831,\n",
       " '奈': 832,\n",
       " '奉': 833,\n",
       " '奋': 834,\n",
       " '奎': 835,\n",
       " '奏': 836,\n",
       " '契': 837,\n",
       " '奔': 838,\n",
       " '奕': 839,\n",
       " '奖': 840,\n",
       " '套': 841,\n",
       " '奘': 842,\n",
       " '奚': 843,\n",
       " '奠': 844,\n",
       " '奢': 845,\n",
       " '奥': 846,\n",
       " '女': 847,\n",
       " '奴': 848,\n",
       " '奶': 849,\n",
       " '奸': 850,\n",
       " '她': 851,\n",
       " '好': 852,\n",
       " '如': 853,\n",
       " '妃': 854,\n",
       " '妄': 855,\n",
       " '妆': 856,\n",
       " '妇': 857,\n",
       " '妈': 858,\n",
       " '妊': 859,\n",
       " '妍': 860,\n",
       " '妒': 861,\n",
       " '妖': 862,\n",
       " '妙': 863,\n",
       " '妞': 864,\n",
       " '妤': 865,\n",
       " '妥': 866,\n",
       " '妧': 867,\n",
       " '妨': 868,\n",
       " '妩': 869,\n",
       " '妮': 870,\n",
       " '妯': 871,\n",
       " '妹': 872,\n",
       " '妻': 873,\n",
       " '姆': 874,\n",
       " '姊': 875,\n",
       " '始': 876,\n",
       " '姐': 877,\n",
       " '姑': 878,\n",
       " '姓': 879,\n",
       " '委': 880,\n",
       " '姗': 881,\n",
       " '姚': 882,\n",
       " '姜': 883,\n",
       " '姝': 884,\n",
       " '姣': 885,\n",
       " '姥': 886,\n",
       " '姨': 887,\n",
       " '姬': 888,\n",
       " '姻': 889,\n",
       " '姿': 890,\n",
       " '威': 891,\n",
       " '娃': 892,\n",
       " '娄': 893,\n",
       " '娅': 894,\n",
       " '娇': 895,\n",
       " '娌': 896,\n",
       " '娘': 897,\n",
       " '娜': 898,\n",
       " '娟': 899,\n",
       " '娠': 900,\n",
       " '娥': 901,\n",
       " '娩': 902,\n",
       " '娱': 903,\n",
       " '娴': 904,\n",
       " '娶': 905,\n",
       " '娼': 906,\n",
       " '婀': 907,\n",
       " '婆': 908,\n",
       " '婉': 909,\n",
       " '婕': 910,\n",
       " '婚': 911,\n",
       " '婧': 912,\n",
       " '婪': 913,\n",
       " '婴': 914,\n",
       " '婵': 915,\n",
       " '婶': 916,\n",
       " '婷': 917,\n",
       " '婿': 918,\n",
       " '媒': 919,\n",
       " '媚': 920,\n",
       " '媛': 921,\n",
       " '媞': 922,\n",
       " '媲': 923,\n",
       " '媳': 924,\n",
       " '嫁': 925,\n",
       " '嫂': 926,\n",
       " '嫉': 927,\n",
       " '嫌': 928,\n",
       " '嫔': 929,\n",
       " '嫖': 930,\n",
       " '嫚': 931,\n",
       " '嫣': 932,\n",
       " '嫦': 933,\n",
       " '嫩': 934,\n",
       " '嬉': 935,\n",
       " '嬛': 936,\n",
       " '嬷': 937,\n",
       " '孀': 938,\n",
       " '子': 939,\n",
       " '孔': 940,\n",
       " '孕': 941,\n",
       " '字': 942,\n",
       " '存': 943,\n",
       " '孙': 944,\n",
       " '孚': 945,\n",
       " '孜': 946,\n",
       " '孝': 947,\n",
       " '孟': 948,\n",
       " '孢': 949,\n",
       " '季': 950,\n",
       " '孤': 951,\n",
       " '学': 952,\n",
       " '孩': 953,\n",
       " '孪': 954,\n",
       " '孰': 955,\n",
       " '孱': 956,\n",
       " '孵': 957,\n",
       " '孺': 958,\n",
       " '宁': 959,\n",
       " '它': 960,\n",
       " '宅': 961,\n",
       " '宇': 962,\n",
       " '守': 963,\n",
       " '安': 964,\n",
       " '宋': 965,\n",
       " '完': 966,\n",
       " '宏': 967,\n",
       " '宓': 968,\n",
       " '宕': 969,\n",
       " '宗': 970,\n",
       " '官': 971,\n",
       " '宙': 972,\n",
       " '定': 973,\n",
       " '宛': 974,\n",
       " '宜': 975,\n",
       " '宝': 976,\n",
       " '实': 977,\n",
       " '宠': 978,\n",
       " '审': 979,\n",
       " '客': 980,\n",
       " '宣': 981,\n",
       " '室': 982,\n",
       " '宦': 983,\n",
       " '宪': 984,\n",
       " '宫': 985,\n",
       " '宰': 986,\n",
       " '害': 987,\n",
       " '宴': 988,\n",
       " '宵': 989,\n",
       " '家': 990,\n",
       " '宸': 991,\n",
       " '容': 992,\n",
       " '宽': 993,\n",
       " '宾': 994,\n",
       " '宿': 995,\n",
       " '寂': 996,\n",
       " '寄': 997,\n",
       " '寅': 998,\n",
       " '密': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254efcbe29f149fabb721708e636bc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=32.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5086519943f04ca494ffe295faa7d536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f21ff1ec7284f5b9bc02b958a5b0967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11259 1433 718\n"
     ]
    }
   ],
   "source": [
    "train_data = {}\n",
    "dev_data = {}\n",
    "test_data = {}\n",
    "\n",
    "for (set_path, dataset) in zip([train_path, dev_path, test_path], [train_data, dev_data, test_data]):\n",
    "    set_dump_path = espnet_path / egs_path / dump_path / set_path / 'deltafalse' \n",
    "    with open(set_dump_path / 'data.json') as fp:\n",
    "        json_data = json.load(fp)\n",
    "\n",
    "    feats = {}\n",
    "    pbar = notebook.tqdm(total=len(list(set_dump_path.glob('feats.*.ark'))))\n",
    "    for feats_file in set_dump_path.glob('feats.*.ark'):\n",
    "        with ReadHelper('ark:'+str(feats_file)) as reader:\n",
    "            for key, numpy_array in reader:\n",
    "                feats[key] = torch.from_numpy(numpy_array)\n",
    "          \n",
    "        pbar.update(1)\n",
    "        \n",
    "        break\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    for key, value in json_data['utts'].items():\n",
    "        if key in feats:\n",
    "            feature = feats[key]\n",
    "            text = json_data['utts'][key]['output'][0]['text']\n",
    "            token = []\n",
    "            token_id = []\n",
    "            for char in text:\n",
    "                if char == ' ':\n",
    "                    token.append('<space>')\n",
    "                else:\n",
    "                    if char in vocab:\n",
    "                        token.append(char)\n",
    "                    else:\n",
    "                        token.append('<unk>')\n",
    "            dataset[key] = {'input':feature,\n",
    "                           'text':text,\n",
    "                           'token':' '.join(token),}\n",
    "        \n",
    "\n",
    "    \n",
    "print(len(train_data), len(dev_data), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "class AttrDict(dict):\n",
    "    \"\"\" Access dictionary keys like attribute \n",
    "        https://stackoverflow.com/questions/4984647/accessing-dict-keys-like-an-attribute\n",
    "    \"\"\"\n",
    "    def __init__(self, *av, **kav):\n",
    "        dict.__init__(self, *av, **kav)\n",
    "        self.__dict__ = self\n",
    "\n",
    "opts = AttrDict()\n",
    "\n",
    "# Configure models\n",
    "\n",
    "opts.ntoken = len(token2id)\n",
    "opts.feature = 83\n",
    "opts.ninp = 256\n",
    "opts.nhead = 4\n",
    "opts.nhid = 2048\n",
    "opts.nlayers_enc = 12\n",
    "opts.nlayers_dec = 6\n",
    "\n",
    "opts.ce_weight = 0.7\n",
    "opts.ctc_weight = 0.3\n",
    "\n",
    "opts.beam_size = 10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configure optimization\n",
    "opts.learning_rate = 5e-5\n",
    "\n",
    "opts.dropout_rate = 0.1\n",
    "\n",
    "opts.batch_size = 64\n",
    "opts.num_workers = int(opts.batch_size / 8) if int(opts.batch_size / 8) < 16 else 16\n",
    "print(opts.num_workers)\n",
    "# Configure training\n",
    "opts.max_seq_len = 512\n",
    "opts.num_epochs = 300\n",
    "# opts.warmup_steps = 4000\n",
    "# opts.gradient_accumulation = 20\n",
    "\n",
    "# opts.load_pretrain = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        \n",
    "        self.names = []\n",
    "        self.features = []\n",
    "        self.texts = []\n",
    "        self.tokens = []\n",
    "        \n",
    "        for name, data in dataset.items():\n",
    "            feature = data['input']\n",
    "            text = data['text']\n",
    "            token = data['token'].split(' ')\n",
    "            token = ['<sos>'] + token + ['<eos>']\n",
    "            \n",
    "            self.names.append(name)\n",
    "            self.features.append(feature)\n",
    "            self.texts.append(text)\n",
    "            self.tokens.append(token)\n",
    "            \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.names[index]\n",
    "        feature = self.features[index]\n",
    "        text = self.texts[index]\n",
    "        token = self.tokens[index]\n",
    "        token_id = self.tokens2ids(token, token2id)\n",
    "        \n",
    "        return name, feature, text, token, token_id\n",
    "    \n",
    "    def tokens2ids(self, tokens, token2id):\n",
    "        token_id = [token2id[token] if token in token2id else token2id['<unk>'] for token in tokens]\n",
    "    \n",
    "        return token_id\n",
    "    \n",
    "    \n",
    "def collate_fn(data):\n",
    "    \n",
    "    def _pad_sequences(seqs):\n",
    "        lens = [len(seq)-1 for seq in seqs]\n",
    "        input_seqs = torch.zeros(len(seqs), max(lens)).long().fill_(token2id['<pad>'])\n",
    "        target_seqs = torch.zeros(len(seqs), max(lens)).long().fill_(token2id['<pad>'])\n",
    "#         input_seqs_mask = input_seqs.float().masked_fill(input_seqs.float()==0, float('-inf'))\n",
    "#         input_seqs_mask = input_seqs_mask.masked_fill(input_seqs_mask!=float('-inf'), 0)\n",
    "\n",
    "        for i, seq in enumerate(seqs):\n",
    "            input_seqs[i, :len(seq)-1] = torch.LongTensor(seq[:-1])\n",
    "            target_seqs[i, :len(seq)-1] = torch.LongTensor(seq[1:])\n",
    "            \n",
    "        input_seqs_mask = input_seqs == token2id['<pad>']\n",
    "        \n",
    "        return input_seqs, input_seqs_mask, target_seqs, lens\n",
    "    \n",
    "    def _pad_features(features):\n",
    "        flens = [len(feature) for feature in features]\n",
    "        input_features = torch.zeros(len(features), max(flens), opts.feature)\n",
    "        for i, feature in enumerate(features):\n",
    "            input_features[i, :len(feature)] = feature\n",
    "            \n",
    "        return input_features\n",
    "    \n",
    "    def _generate_square_subsequent_mask(sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = ~mask\n",
    "        \n",
    "        return mask\n",
    "        \n",
    "    \n",
    "    name, feature, text, token, token_id = zip(*data)\n",
    "    \n",
    "    input_seqs, input_seqs_pad_mask, target_seqs, lens = _pad_sequences(token_id)\n",
    "    \n",
    "    input_features = _pad_features(feature)\n",
    "    \n",
    "    input_seqs_mask = _generate_square_subsequent_mask(input_seqs.size(1))\n",
    "    input_seqs_mask = input_seqs_mask.repeat(input_seqs.size(0), 1, 1) #為了給dataparallel切 要給他一個batch維\n",
    "    \n",
    "    lens = torch.LongTensor(lens)\n",
    "    \n",
    "    return name, input_features, text, token, input_seqs, input_seqs_mask, input_seqs_pad_mask, target_seqs, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data)\n",
    "dev_dataset = Dataset(dev_data)\n",
    "test_dataset = Dataset(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BAC009S0728W0126', tensor([[-0.6655, -0.6084, -0.7678,  ...,  1.4794,  2.3732, -0.1098],\n",
      "        [-0.4260, -1.0659, -0.8746,  ...,  0.3665,  2.3732,  0.0944],\n",
      "        [ 0.3370,  0.1265, -0.8746,  ...,  0.7317,  2.3732,  0.0718],\n",
      "        ...,\n",
      "        [-0.0736, -0.4559, -1.7645,  ...,  1.7496,  1.9711,  0.1323],\n",
      "        [ 0.1625, -0.1001, -1.1237,  ...,  0.6556,  1.9711, -0.0341],\n",
      "        [ 0.2036, -0.1001, -0.9458,  ...,  1.3593,  1.9459,  0.0566]]), '必然先行抛售二三线城市的房产', ['<sos>', '必', '然', '先', '行', '抛', '售', '二', '三', '线', '城', '市', '的', '房', '产', '<eos>'], [2, 1249, 2306, 264, 3366, 1437, 628, 73, 10, 2888, 767, 1124, 2556, 1396, 88, 3])\n"
     ]
    }
   ],
   "source": [
    "print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20200915)\n",
    "torch.manual_seed(20200915)\n",
    "torch.cuda.manual_seed_all(20200915)\n",
    "\n",
    "train_iter = DataLoader(dataset=train_dataset,\n",
    "                        batch_size=opts.batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=opts.num_workers,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "dev_iter = DataLoader(dataset=dev_dataset,\n",
    "                        batch_size=opts.batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=opts.num_workers,\n",
    "                        collate_fn=collate_fn)\n",
    "\n",
    "test_iter = DataLoader(dataset=test_dataset,\n",
    "                        batch_size=opts.batch_size,\n",
    "                        shuffle=False,\n",
    "                        num_workers=opts.num_workers,\n",
    "                        collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from typing import Optional, Any\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.modules.activation import MultiheadAttention\n",
    "from torch.nn.modules.container import ModuleList\n",
    "from torch.nn.init import xavier_uniform_\n",
    "from torch.nn.modules.dropout import Dropout\n",
    "from torch.nn.modules.linear import Linear\n",
    "from torch.nn.modules.normalization import LayerNorm\n",
    "\n",
    "class TransformerEncoderLayer(Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout = Dropout(dropout)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerEncoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Optional[Tensor] = None, src_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        \n",
    "        src2 = self.norm1(src)\n",
    "        src2 = self.self_attn(src2, src2, src2, attn_mask=src_mask,\n",
    "                              key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        \n",
    "        src2 = self.norm2(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))\n",
    "        src = src + self.dropout2(src2)\n",
    "\n",
    "        return src\n",
    "\n",
    "class TransformerDecoderLayer(Module):\n",
    "    r\"\"\"TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.\n",
    "    This standard decoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of intermediate layer, relu or gelu (default=relu).\n",
    "\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = decoder_layer(tgt, memory)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"):\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "        self.self_attn_m = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.self_attn_t = MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1_m = Linear(d_model, dim_feedforward)\n",
    "        self.dropout_m = Dropout(dropout)\n",
    "        self.linear2_m = Linear(dim_feedforward, d_model)\n",
    "        \n",
    "        self.linear1_t = Linear(d_model, dim_feedforward)\n",
    "        self.dropout_t = Dropout(dropout)\n",
    "        self.linear2_t = Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.activation = _get_activation_fn(activation)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        if 'activation' not in state:\n",
    "            state['activation'] = F.relu\n",
    "        super(TransformerDecoderLayer, self).__setstate__(state)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None, memory_mask: Optional[Tensor] = None,\n",
    "                tgt_key_padding_mask: Optional[Tensor] = None, memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder layer (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        \n",
    "        m_cat_t = torch.cat([memory, tgt], dim=0)\n",
    "        m_cat_t = self.norm1(m_cat_t)\n",
    "        \n",
    "        memory = m_cat_t[:memory.size(0), :, :]\n",
    "        tgt = m_cat_t[memory.size(0):, :, :]\n",
    "        \n",
    "        memory2 = self.self_attn_m(memory, memory, memory, attn_mask=None,\n",
    "                                 key_padding_mask=None)[0]\n",
    "        \n",
    "        m_cat_t_mask = torch.BoolTensor(tgt.size(0), m_cat_t.size(0)).fill_(False).to(tgt.device)\n",
    "        m_cat_t_mask[:, -tgt_mask.size(0):] = tgt_mask\n",
    "        m_cat_t_pad_mask = torch.BoolTensor(tgt_key_padding_mask.size(0), m_cat_t.size(0)).fill_(False).to(tgt.device)\n",
    "        m_cat_t_pad_mask[:, -tgt_key_padding_mask.size(1):] = tgt_key_padding_mask\n",
    "        \n",
    "        \n",
    "#         print(tgt.shape)\n",
    "#         print(tgt_key_padding_mask.shape)\n",
    "#         print(m_cat_t.shape)\n",
    "#         print(m_cat_t_mask.shape)\n",
    "        tgt2 = self.self_attn_t(tgt, m_cat_t, m_cat_t, attn_mask=m_cat_t_mask,\n",
    "                               key_padding_mask=m_cat_t_pad_mask)[0]\n",
    "        \n",
    "        memory = memory + self.dropout1(memory2)\n",
    "        tgt = tgt + self.dropout1(tgt2)\n",
    "        \n",
    "        m_cat_t = torch.cat([memory, tgt], dim=0)\n",
    "        m_cat_t = self.norm2(m_cat_t)\n",
    "        \n",
    "        memory = m_cat_t[:memory.size(0), :, :]\n",
    "        tgt = m_cat_t[memory.size(0):, :, :]\n",
    "        \n",
    "        memory2 = self.linear2_m(self.dropout_m(self.activation(self.linear1_m(memory))))\n",
    "        tgt2 = self.linear2_t(self.dropout_t(self.activation(self.linear1_t(tgt))))\n",
    "        \n",
    "        memory = memory + self.dropout2(memory2)\n",
    "        tgt = tgt + self.dropout2(tgt2)\n",
    "        \n",
    "        return memory, tgt\n",
    "    \n",
    "def _get_clones(module, N):\n",
    "    return ModuleList([copy.deepcopy(module) for i in range(N)])\n",
    "\n",
    "\n",
    "def _get_activation_fn(activation):\n",
    "    if activation == \"relu\":\n",
    "        return F.relu\n",
    "    elif activation == \"gelu\":\n",
    "        return F.gelu\n",
    "\n",
    "    raise RuntimeError(\"activation should be relu/gelu, not {}\".format(activation))\n",
    "    \n",
    "class TransformerDecoder(Module):\n",
    "    r\"\"\"TransformerDecoder is a stack of N decoder layers\n",
    "\n",
    "    Args:\n",
    "        decoder_layer: an instance of the TransformerDecoderLayer() class (required).\n",
    "        num_layers: the number of sub-decoder-layers in the decoder (required).\n",
    "        norm: the layer normalization component (optional).\n",
    "\n",
    "    Examples::\n",
    "        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)\n",
    "        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)\n",
    "        >>> memory = torch.rand(10, 32, 512)\n",
    "        >>> tgt = torch.rand(20, 32, 512)\n",
    "        >>> out = transformer_decoder(tgt, memory)\n",
    "    \"\"\"\n",
    "    __constants__ = ['norm']\n",
    "\n",
    "    def __init__(self, decoder_layer, num_layers, norm=None):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.layers = _get_clones(decoder_layer, num_layers)\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = norm\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = None,\n",
    "                memory_mask: Optional[Tensor] = None, tgt_key_padding_mask: Optional[Tensor] = None,\n",
    "                memory_key_padding_mask: Optional[Tensor] = None) -> Tensor:\n",
    "        r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n",
    "\n",
    "        Args:\n",
    "            tgt: the sequence to the decoder (required).\n",
    "            memory: the sequence from the last layer of the encoder (required).\n",
    "            tgt_mask: the mask for the tgt sequence (optional).\n",
    "            memory_mask: the mask for the memory sequence (optional).\n",
    "            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n",
    "            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "        output = tgt\n",
    "\n",
    "        for mod in self.layers:\n",
    "            memory, output = mod(output, memory, tgt_mask=tgt_mask,\n",
    "                                 memory_mask=memory_mask,\n",
    "                                 tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                 memory_key_padding_mask=memory_key_padding_mask)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            output = self.norm(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dSubsampling(torch.nn.Module):\n",
    "    \"\"\"Convolutional 2D subsampling (to 1/4 length).\n",
    "    :param int idim: input dim\n",
    "    :param int odim: output dim\n",
    "    :param flaot dropout_rate: dropout rate\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, odim, dropout_rate=0.5):\n",
    "        \"\"\"Construct an Conv2dSubsampling object.\"\"\"\n",
    "        super(Conv2dSubsampling, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, odim, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(odim, odim, 3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "        self.out = torch.nn.Sequential(\n",
    "            torch.nn.Linear(odim * (((idim - 1) // 2 - 1) // 2), odim),\n",
    "#             PositionalEncoding(odim, dropout_rate),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        \"\"\"Subsample x.\n",
    "        :param torch.Tensor x: input tensor\n",
    "        :param torch.Tensor x_mask: input mask\n",
    "        :return: subsampled x and mask\n",
    "        :rtype Tuple[torch.Tensor, torch.Tensor]\n",
    "        \"\"\"\n",
    "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
    "        x = self.conv(x)\n",
    "        b, c, t, f = x.size()\n",
    "        x = self.out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
    "        if x_mask is None:\n",
    "            return x, None\n",
    "        return x, x_mask[:, :, :-2:2][:, :, :-2:2]\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class EncoderModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, idim, dropout=0.5):\n",
    "        super(EncoderModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        \n",
    "        self.embedding = Conv2dSubsampling(idim, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.layer_norm = LayerNorm(ninp)\n",
    "        \n",
    "        self.ninp = ninp\n",
    "#         self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "#         self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        # src [b, t, f]        \n",
    "        \n",
    "        src, self.src_mask = self.embedding(src, self.src_mask)\n",
    "\n",
    "        src = src.permute(1, 0, 2) # [t, b, f]\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src=src, \n",
    "                                          mask=self.src_mask,\n",
    "                                         )\n",
    "#         output = self.decoder(output)\n",
    "        output = self.layer_norm(output)\n",
    "    \n",
    "        return output\n",
    "\n",
    "class DecoderModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(DecoderModel, self).__init__()\n",
    "#         from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.tgt_mask = None\n",
    "        self.memory_mask = None\n",
    "        \n",
    "        self.embedding = nn.Embedding(ntoken, ninp)\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "        self.layer_norm = LayerNorm(ninp)\n",
    "        \n",
    "        self.ninp = ninp\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "    \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def forward(self, tgt, tgt_mask, tgt_key_padding_mask, memory):\n",
    "        \n",
    "        # tgt [b, t], tgt_key_padding_mask [b, t], memory [t, b, f]\n",
    "        \n",
    "        tgt = tgt.permute(1, 0) # [t, b]\n",
    "        \n",
    "#         if self.tgt_mask is None or self.tgt_mask.size(0) != len(tgt):\n",
    "#             device = tgt.device\n",
    "#             mask = self._generate_square_subsequent_mask(len(tgt)).to(device)\n",
    "#             self.tgt_mask = mask\n",
    "\n",
    "        # The reason we increase the embedding values before the addition \n",
    "        # is to make the positional encoding relatively smaller. \n",
    "        # This means the original meaning \n",
    "        # in the embedding vector won’t be lost when we add them together.\n",
    "        # maybe use learned position embedding will not need to do this?  \n",
    "        \n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.ninp)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        \n",
    "        tgt_mask = tgt_mask[0] ##為了dataparallel給的batch維把它去掉\n",
    "        \n",
    "        output = self.transformer_decoder(tgt, memory, \n",
    "                                          tgt_mask=tgt_mask, \n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask, \n",
    "                                          memory_mask=self.memory_mask, \n",
    "                                          memory_key_padding_mask=None,\n",
    "                                          )\n",
    "        \n",
    "        output = self.layer_norm(output)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers_enc, nlayers_dec, idim, dropout=0.5):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.encoder = EncoderModel(ntoken, ninp, nhead, nhid, nlayers_enc, idim, dropout)\n",
    "        self.ctc_classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.decoder = DecoderModel(ntoken, ninp, nhead, nhid, nlayers_dec, dropout)\n",
    "        self.classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "    def forward(self, src, tgt, tgt_mask, tgt_key_padding_mask):\n",
    "        \n",
    "        memory = self.encoder(src)\n",
    "        \n",
    "        ctc_output = self.ctc_classifier(memory.permute(1, 0, 2))\n",
    "        \n",
    "        decoder_output = self.decoder(tgt, tgt_mask, tgt_key_padding_mask, memory)\n",
    "        \n",
    "        output = self.classifier(decoder_output.permute(1, 0, 2))\n",
    "\n",
    "        # return 一定要batch first 不然dataparallel會concat錯維\n",
    "        return ctc_output, output, memory.permute(1, 0, 2), decoder_output.permute(1, 0, 2)\n",
    "#         return memory, ctc_output, decoder_output\n",
    "\n",
    "\n",
    "class Test_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers_enc, nlayers_dec, idim, dropout=0.5):\n",
    "        super(Test_Model, self).__init__()\n",
    "        \n",
    "        self.encoder = EncoderModel(ntoken, ninp, nhead, nhid, nlayers_enc, idim, dropout)\n",
    "        self.ctc_classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.decoder = DecoderModel(ntoken, ninp, nhead, nhid, nlayers_dec, dropout)\n",
    "        self.classifier = nn.Linear(ninp, ntoken)\n",
    "        \n",
    "        self.encoder.eval()\n",
    "        self.ctc_classifier.eval()\n",
    "        self.decoder.eval()\n",
    "        self.classifier.eval()\n",
    "        \n",
    "    def forward(self, src, len_limit):\n",
    "        \n",
    "        device = src.device\n",
    "        memory = self.encoder(src)\n",
    "        \n",
    "        nbest = []\n",
    "        beams = []\n",
    "        beams.append([[token2id['<sos>']], 0])\n",
    "\n",
    "        for _ in range(2*len_limit):\n",
    "\n",
    "            results = []\n",
    "\n",
    "            for beam in beams:\n",
    "\n",
    "                input_idxs = beam[0]\n",
    "\n",
    "                input_seqs = torch.LongTensor([input_idxs])\n",
    "        #         input_seqs = input_seqs.unsqueeze(0)\n",
    "                input_seqs = input_seqs.to(device)\n",
    "\n",
    "                decoder_output = self.decoder(input_seqs, None, memory)\n",
    "                output = self.classifier(decoder_output.permute(1, 0, 2)[:, -1])\n",
    "                output = output.log_softmax(dim=1)\n",
    "                output[:, 0] = float('-inf')\n",
    "                probs, idxs = output.topk(k=opts.beam_size, dim=1)\n",
    "\n",
    "                for prob, idx in zip(probs.squeeze(0), idxs.squeeze(0)):\n",
    "\n",
    "                    generate_idxs = input_idxs + [idx.item()]\n",
    "                    accumulate_prob = beam[1] + prob.item()\n",
    "\n",
    "                    results.append([generate_idxs, accumulate_prob])\n",
    "\n",
    "\n",
    "            results.sort(key=lambda x:x[1])\n",
    "            results = results[::-1]\n",
    "            results = results[:3]\n",
    "\n",
    "            beams = []\n",
    "\n",
    "            for result in results:\n",
    "                if result[0][-1] == token2id['<eos>']:\n",
    "                    nbest.append(result)\n",
    "                else:\n",
    "                    beams.append(result)\n",
    "\n",
    "        return nbest, beams\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/disk3/m10615110/espnet/egs/aishell/asr1/exp\n",
      "/mnt/disk3/m10615110/espnet/egs/aishell/asr1/exp/myaishell_selfmix_ce0.7_ctc0.3_2020-09-22 16:12:42\n"
     ]
    }
   ],
   "source": [
    "print(exp_dir)\n",
    "\n",
    "RESTORE = False\n",
    "\n",
    "LOAD_RNNLM = False\n",
    "\n",
    "if RESTORE:\n",
    "    experiment_dir = Path(exp_dir) / 'rnnlm_1stlayer_BERT_EN_finetune_increment_hidden300_len35_2020-01-01 11:15:21'\n",
    "    last_epoch = 29\n",
    "    print(experiment_dir)\n",
    "    \n",
    "else:\n",
    "\n",
    "    if LOAD_RNNLM:\n",
    "        experiment_name = 'rnnlm_dictALL_talk-sent_len130_2019-11-09 14:53:52'\n",
    "        experiment_dir = Path(exp_dir) / experiment_name\n",
    "        model_dir = experiment_dir / 'best_model'\n",
    "        print(model_dir)\n",
    "\n",
    "    last_epoch = -1\n",
    "    model_name = 'myaishell_selfmix_ce{}_ctc{}'.format(\\\n",
    "                                   opts.ce_weight, opts.ctc_weight)\n",
    "    now = str(datetime.now()).split('.')[0]\n",
    "    experiment_name = '{}_{}'.format(model_name, now)\n",
    "    experiment_dir = Path(exp_dir) / experiment_name\n",
    "    experiment_dir.mkdir(exist_ok=True, parents=True)\n",
    "    print(experiment_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_trainlog = experiment_dir / 'train_log.txt'\n",
    "\n",
    "def log2file(log_file, msg):\n",
    "    with open(log_file, 'a') as fw:\n",
    "        fw.write(msg)\n",
    "        fw.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parms :  36724246\n",
      "trainable parms :  36724246\n"
     ]
    }
   ],
   "source": [
    "model = Model(opts.ntoken, opts.ninp, opts.nhead, opts.nhid, opts.nlayers_enc, opts.nlayers_dec, opts.feature, opts.dropout_rate)\n",
    "\n",
    "print('total parms : ', sum(p.numel() for p in model.parameters()))\n",
    "print('trainable parms : ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 2 GPUs!\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False\n",
    "\n",
    "print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CUDA:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=opts.learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=opts.learning_rate, steps_per_epoch=len(train_iter), epochs=20)\n",
    "\n",
    "# ce_criterion = torch.nn.KLDivLoss()\n",
    "ce_criterion = torch.nn.CrossEntropyLoss(reduction='mean', ignore_index=token2id['<pad>'])\n",
    "ctc_criterion = torch.nn.CTCLoss(reduction='mean', blank=token2id['<blank>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_epoch = -1\n",
    "\n",
    "for k,v in opts.items():\n",
    "    log_msg = '- {}: {}'.format(k, v)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    print(log_msg)\n",
    "\n",
    "pbar_train = notebook.tqdm(total=len(train_iter))\n",
    "pbar_dev = notebook.tqdm(total=len(dev_iter))\n",
    "pbar_test = notebook.tqdm(total=len(test_iter))\n",
    "\n",
    "log_msg = '='*50\n",
    "print(log_msg)\n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "log_msg = 'optim : \\n' + str(optimizer)\n",
    "print(log_msg)   \n",
    "log2file(str(experiment_trainlog), log_msg)\n",
    "\n",
    "for epoch in range(last_epoch+1,  opts.num_epochs, 1):\n",
    "    \n",
    "    pbar_train.reset()\n",
    "    pbar_dev.reset()\n",
    "    pbar_test.reset()\n",
    "    \n",
    "    loss_tracker = []\n",
    "    celoss_tracker = []\n",
    "    ctcloss_tracker = []\n",
    "    time_tracker = []\n",
    "    time_tracker.append(time.time())\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for iteration, batch in enumerate(train_iter):\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        name, input_features, text, token, input_seqs, input_seqs_mask, input_seqs_pad_mask, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_features.size(0)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_features = input_features.cuda()\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_seqs_mask = input_seqs_mask.cuda()\n",
    "            input_seqs_pad_mask = input_seqs_pad_mask.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        ctc_output, output, memory, decoder_output = model(input_features, input_seqs, input_seqs_mask, input_seqs_pad_mask)\n",
    "        \n",
    "        total += (target_seqs.view(-1) != token2id['<pad>']).sum().item()\n",
    "        _, predicted = torch.max(output.view(-1, opts.ntoken).data, 1)\n",
    "        correct += ((predicted == target_seqs.view(-1)) * (target_seqs.view(-1) != token2id['<pad>'])).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ce_loss = ce_criterion(output.view(-1, opts.ntoken), target_seqs.view(-1))\n",
    "        \n",
    "        ctc_output = ctc_output.permute(1, 0, 2).log_softmax(2)\n",
    "        \n",
    "        input_lengths = torch.full(size=(ctc_output.size(1),), fill_value=ctc_output.size(0), dtype=torch.long)\n",
    "        target_lengths = lens\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_lengths = input_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "        \n",
    "        ctc_loss = ctc_criterion(ctc_output, target_seqs, input_lengths, target_lengths)\n",
    "        \n",
    "        loss = opts.ce_weight*ce_loss + opts.ctc_weight*ctc_loss\n",
    "        \n",
    "        celoss_tracker.append(ce_loss.item()*batch_size)\n",
    "        ctcloss_tracker.append(ctc_loss.item()*batch_size)\n",
    "        loss_tracker.append(loss.item()*batch_size)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        pbar_train.update(1)\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    time_tracker.append(time.time())\n",
    "    log_msg = \"{} | Epoch {:d}/{:d} | Mean CE / CTC / ALL Loss {:5.2f} / {:5.2f} / {:5.2f} | acc {:5.5f} % | time cost {:d} s\"\\\n",
    "            .format('train'.upper(), epoch, opts.num_epochs, \\\n",
    "                    np.mean(celoss_tracker), np.mean(ctcloss_tracker), np.mean(loss_tracker), \\\n",
    "                    float(correct)/float(total)*100, int(time_tracker[-1] - time_tracker[-2]))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    celoss_tracker = []\n",
    "    ctcloss_tracker = []\n",
    "    time_tracker = []\n",
    "    time_tracker.append(time.time())\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for iteration, batch in enumerate(dev_iter):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        name, input_features, text, token, input_seqs, input_seqs_mask, input_seqs_pad_mask, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_features.size(0)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_features = input_features.cuda()\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_seqs_mask = input_seqs_mask.cuda()\n",
    "            input_seqs_pad_mask = input_seqs_pad_mask.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        ctc_output, output, memory, decoder_output = model(input_features, input_seqs, input_seqs_mask, input_seqs_pad_mask)\n",
    "        \n",
    "        total += (target_seqs.view(-1) != token2id['<pad>']).sum().item()\n",
    "        _, predicted = torch.max(output.view(-1, opts.ntoken).data, 1)\n",
    "        correct += ((predicted == target_seqs.view(-1)) * (target_seqs.view(-1) != token2id['<pad>'])).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ce_loss = ce_criterion(output.view(-1, opts.ntoken), target_seqs.view(-1))\n",
    "        \n",
    "        ctc_output = ctc_output.permute(1, 0, 2).log_softmax(2)\n",
    "        \n",
    "        input_lengths = torch.full(size=(ctc_output.size(1),), fill_value=ctc_output.size(0), dtype=torch.long)\n",
    "        target_lengths = lens\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_lengths = input_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "        \n",
    "        ctc_loss = ctc_criterion(ctc_output, target_seqs, input_lengths, target_lengths)\n",
    "        \n",
    "        loss = opts.ce_weight*ce_loss + opts.ctc_weight*ctc_loss\n",
    "        \n",
    "        celoss_tracker.append(ce_loss.item()*batch_size)\n",
    "        ctcloss_tracker.append(ctc_loss.item()*batch_size)\n",
    "        loss_tracker.append(loss.item()*batch_size)\n",
    "        \n",
    "        pbar_dev.update(1)\n",
    "        \n",
    "    time_tracker.append(time.time())\n",
    "    log_msg = \"{} | Epoch {:d}/{:d} | Mean CE / CTC / ALL Loss {:5.2f} / {:5.2f} / {:5.2f} | acc {:5.5f} % | time cost {:d} s\"\\\n",
    "            .format('dev  '.upper(), epoch, opts.num_epochs, \\\n",
    "                    np.mean(celoss_tracker), np.mean(ctcloss_tracker), np.mean(loss_tracker), \\\n",
    "                    float(correct)/float(total)*100, int(time_tracker[-1] - time_tracker[-2]))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    loss_tracker = []\n",
    "    celoss_tracker = []\n",
    "    ctcloss_tracker = []\n",
    "    time_tracker = []\n",
    "    time_tracker.append(time.time())\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for iteration, batch in enumerate(test_iter):\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        name, input_features, text, token, input_seqs, input_seqs_mask, input_seqs_pad_mask, target_seqs, lens = batch\n",
    "        \n",
    "        batch_size = input_features.size(0)\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_features = input_features.cuda()\n",
    "            input_seqs = input_seqs.cuda()\n",
    "            input_seqs_mask = input_seqs_mask.cuda()\n",
    "            input_seqs_pad_mask = input_seqs_pad_mask.cuda()\n",
    "            target_seqs = target_seqs.cuda()\n",
    "        \n",
    "        ctc_output, output, memory, decoder_output = model(input_features, input_seqs, input_seqs_mask, input_seqs_pad_mask)\n",
    "        \n",
    "        total += (target_seqs.view(-1) != token2id['<pad>']).sum().item()\n",
    "        _, predicted = torch.max(output.view(-1, opts.ntoken).data, 1)\n",
    "        correct += ((predicted == target_seqs.view(-1)) * (target_seqs.view(-1) != token2id['<pad>'])).sum().item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ce_loss = ce_criterion(output.view(-1, opts.ntoken), target_seqs.view(-1))\n",
    "        \n",
    "        ctc_output = ctc_output.permute(1, 0, 2).log_softmax(2)\n",
    "        \n",
    "        input_lengths = torch.full(size=(ctc_output.size(1),), fill_value=ctc_output.size(0), dtype=torch.long)\n",
    "        target_lengths = lens\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            input_lengths = input_lengths.cuda()\n",
    "            target_lengths = target_lengths.cuda()\n",
    "        \n",
    "        ctc_loss = ctc_criterion(ctc_output, target_seqs, input_lengths, target_lengths)\n",
    "        \n",
    "        loss = opts.ce_weight*ce_loss + opts.ctc_weight*ctc_loss\n",
    "        \n",
    "        celoss_tracker.append(ce_loss.item()*batch_size)\n",
    "        ctcloss_tracker.append(ctc_loss.item()*batch_size)\n",
    "        loss_tracker.append(loss.item()*batch_size)\n",
    "        \n",
    "        pbar_test.update(1)\n",
    "        \n",
    "    time_tracker.append(time.time())\n",
    "    log_msg = \"{} | Epoch {:d}/{:d} | Mean CE / CTC / ALL Loss {:5.2f} / {:5.2f} / {:5.2f} | acc {:5.5f} % | time cost {:d} s\"\\\n",
    "            .format('test '.upper(), epoch, opts.num_epochs, \\\n",
    "                    np.mean(celoss_tracker), np.mean(ctcloss_tracker), np.mean(loss_tracker), \\\n",
    "                    float(correct)/float(total)*100, int(time_tracker[-1] - time_tracker[-2]))\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    checkpoint = {\n",
    "        \"net\": model.state_dict(),\n",
    "        'optimizer':optimizer.state_dict(),\n",
    "        \"epoch\": epoch\n",
    "    }\n",
    "    \n",
    "    torch.save(checkpoint, experiment_dir / 'epoch_{}.ckpt'.format(epoch))\n",
    "    \n",
    "    log_msg = '='*50\n",
    "    print(log_msg)\n",
    "    log2file(str(experiment_trainlog), log_msg)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
